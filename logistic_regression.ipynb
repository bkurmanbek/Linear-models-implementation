{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression and key insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Descent to Binary Logistic Regression \n",
    "class GDLogisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, tolerance=1e-8, regularization=None, alpha=0.01, l1_ratio=0.5, max_iter=100):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.tolerance = tolerance\n",
    "        self.regularization = regularization\n",
    "        self.alpha = alpha\n",
    "        self.l1_ratio = l1_ratio\n",
    "        self.max_iter = max_iter\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def binary_crossentropy(self, y_true, y_pred):\n",
    "        # Binary cross-entropy loss\n",
    "        y_pred = np.clip(y_pred, 1e-10, 1 - 1e-10)  \n",
    "        return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "    \n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        self.bias = 0\n",
    "        self.weights = np.zeros(n_features)\n",
    "\n",
    "        previous_db = 0\n",
    "        previous_dw = np.zeros(n_features)\n",
    "\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "\n",
    "        for _ in range(self.max_iter):\n",
    "        \n",
    "            y_pred_linear = X @ self.weights + self.bias\n",
    "            y_pred_sigmoid = self.sigmoid(y_pred_linear)\n",
    "            \n",
    "            # Gradients \n",
    "            db = 1 / n_samples * np.sum(y_pred_sigmoid - y)\n",
    "            dw = 1 / n_samples * X.T @ (y_pred_sigmoid - y)\n",
    "\n",
    "            if self.regularization == \"ridge\":\n",
    "                dw += (self.alpha / n_samples) * self.weights\n",
    "            elif self.regularization == \"lasso\":\n",
    "                dw += (self.alpha / n_samples) * np.sign(self.weights)\n",
    "            elif self.regularization == \"elasticnet\":\n",
    "                dw += (self.alpha / n_samples) * (self.l1_ratio * np.sign(self.weights) + (1 - self.l1_ratio) * self.weights)\n",
    "\n",
    "            self.bias -= self.learning_rate * db\n",
    "            self.weights -= self.learning_rate * dw\n",
    "\n",
    "            train_loss = self.binary_crossentropy(y, y_pred_sigmoid)\n",
    "            self.train_losses.append(train_loss)\n",
    "\n",
    "            if X_val is not None and y_val is not None:\n",
    "                y_val_pred_linear = X_val @ self.weights + self.bias\n",
    "                y_val_pred_sigmoid = self.sigmoid(y_val_pred_linear)\n",
    "                val_loss = self.binary_crossentropy(y_val, y_val_pred_sigmoid)\n",
    "                self.val_losses.append(val_loss)\n",
    "\n",
    "            abs_db_reduction = np.abs(db - previous_db)\n",
    "            abs_dw_reduction = np.abs(dw - previous_dw)\n",
    "\n",
    "            if abs_db_reduction < self.tolerance and abs_dw_reduction.all() < self.tolerance:\n",
    "                break\n",
    "\n",
    "            previous_db = db\n",
    "            previous_dw = dw\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        y_pred_linear = X_test @ self.weights + self.bias \n",
    "        y_pred_sigmoid = self.sigmoid(y_pred_linear)\n",
    "\n",
    "        classes = np.array([0 if pred < 0.5 else 1 for pred in y_pred_sigmoid])\n",
    "        return classes\n",
    "    \n",
    "    def evaluate(self, X_test, y_test):\n",
    "        y_pred = self.predict(X_test)\n",
    "\n",
    "        accuracy = np.mean(y_pred == y_test)\n",
    "        precision = np.sum((y_pred == 1) & (y_test == 1)) / np.sum(y_pred == 1)\n",
    "        recall = np.sum((y_pred == 1) & (y_test == 1)) / np.sum(y_test == 1)\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "        return {\n",
    "            \"accuracy\": accuracy,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1_score\": f1_score\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SoftmaxRegression:\n",
    "    def __init__(self, learning_rate=0.1, tolerance=1e-4, max_iter=1000, \n",
    "                 regularization=None, alpha=0.01, l1_ratio=0.5):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.tolerance = tolerance\n",
    "        self.max_iter = max_iter\n",
    "        self.regularization = regularization\n",
    "        self.alpha = alpha\n",
    "        self.l1_ratio = l1_ratio\n",
    "\n",
    "    def softmax(self, predictions):\n",
    "        exp = np.exp(predictions - np.max(predictions, axis=1, keepdims=True))\n",
    "        return exp / np.sum(exp, axis=1, keepdims=True)\n",
    "    \n",
    "    def cross_entropy_loss(self, y_true, y_pred):\n",
    "        y_pred = np.clip(y_pred, 1e-10, 1 - 1e-10)  # Avoiding log(0)\n",
    "        return -np.mean(np.sum(y_true * np.log(y_pred), axis=1))\n",
    "\n",
    "    def one_hot_encode(self, y, n_classes):\n",
    "        one_hot = np.zeros((y.size, n_classes))\n",
    "        one_hot[np.arange(y.size), y] = 1\n",
    "        return one_hot\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_classes = len(np.unique(y))\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        one_hot_y = self.one_hot_encode(y, n_classes)\n",
    "\n",
    "        self.bias = np.zeros(n_classes)\n",
    "        self.weights = np.zeros((n_features, n_classes))\n",
    "\n",
    "        previous_db = np.zeros(n_classes)\n",
    "        previous_dw = np.zeros((n_features, n_classes))\n",
    "\n",
    "        self.losses = []\n",
    "\n",
    "        for _ in range(self.max_iter):\n",
    "            y_pred_linear = X @ self.weights + self.bias\n",
    "            y_pred_softmax = self.softmax(y_pred_linear)\n",
    "\n",
    "            db = (1 / n_samples) * np.sum(y_pred_softmax - one_hot_y, axis=0)\n",
    "            dw = (1 / n_samples) * X.T @ (y_pred_softmax - one_hot_y)\n",
    "\n",
    "            if self.regularization == \"ridge\":  \n",
    "                dw += (self.alpha / n_samples) * self.weights\n",
    "            elif self.regularization == \"lasso\":  \n",
    "                dw += (self.alpha / n_samples) * np.sign(self.weights)\n",
    "            elif self.regularization == \"elasticnet\": \n",
    "                l1_term = self.l1_ratio * np.sign(self.weights)\n",
    "                l2_term = (1 - self.l1_ratio) * self.weights\n",
    "                dw += (self.alpha / n_samples) * (l1_term + l2_term)\n",
    "\n",
    "            self.bias -= self.learning_rate * db\n",
    "            self.weights -= self.learning_rate * dw\n",
    "\n",
    "            # loss (including regularization in loss calculation)\n",
    "            loss = self.cross_entropy_loss(one_hot_y, y_pred_softmax)\n",
    "            if self.regularization == \"ridge\":\n",
    "                loss += (self.alpha / (2 * n_samples)) * np.sum(self.weights ** 2)\n",
    "            elif self.regularization == \"lasso\":\n",
    "                loss += (self.alpha / n_samples) * np.sum(np.abs(self.weights))\n",
    "            elif self.regularization == \"elasticnet\":\n",
    "                l1_loss = self.l1_ratio * np.sum(np.abs(self.weights))\n",
    "                l2_loss = (1 - self.l1_ratio) * np.sum(self.weights ** 2)\n",
    "                loss += (self.alpha / n_samples) * (l1_loss + l2_loss)\n",
    "\n",
    "            self.losses.append(loss)\n",
    "\n",
    "            abs_db_reduction = np.abs(db - previous_db)\n",
    "            abs_dw_reduction = np.abs(dw - previous_dw)\n",
    "\n",
    "            if np.all(abs_db_reduction < self.tolerance) and np.all(abs_dw_reduction < self.tolerance):\n",
    "                break\n",
    "\n",
    "            previous_db = db\n",
    "            previous_dw = dw\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        y_pred_linear = X_test @ self.weights + self.bias\n",
    "        y_pred_softmax = self.softmax(y_pred_linear)\n",
    "        most_prob_class = np.argmax(y_pred_softmax, axis=1)\n",
    "        return most_prob_class\n",
    "\n",
    "    def evaluate(self, X_test, y_test):\n",
    "        y_pred = self.predict(X_test)\n",
    "        accuracy = np.mean(y_pred == y_test)\n",
    "        return accuracy\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
